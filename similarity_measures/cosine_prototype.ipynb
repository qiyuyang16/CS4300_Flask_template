{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd004ac14935f6ed29b3349ee8f41114d2dfa2ba78ce87cf701ad9b7ca15955b787",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(txt_file):\n",
    "    \"\"\"\n",
    "    [txt_file]: path to txt file\n",
    "    return:\n",
    "        Dict{page_num: page_text_string}\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    with io.open(txt_file, encoding = 'utf8') as f:\n",
    "        txt_string = f.read()\n",
    "    f.close()\n",
    "    for page in txt_string.split('\\n\\n'):\n",
    "        doc = re.sub('\\n', ' ', page)\n",
    "        if (doc != '') and (not doc.isspace()): # TODO doc_idx needs to correspond to pdf page_num\n",
    "            docs.append(doc)\n",
    "    docs = dict(enumerate(docs, start = 1))\n",
    "    return docs\n",
    "\n",
    "pages = get_pages('../streamlit_testing/pdftotext_result.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(pages):\n",
    "    \"\"\"\n",
    "    Pre-process the raw pages\n",
    "\n",
    "    [pages]: Dict{page_num: page_text_string}\n",
    "    return:\n",
    "        Dict{page_num: page_text_string}\n",
    "    \"\"\"\n",
    "    processed_docs = {}\n",
    "    for i in pages.keys():\n",
    "        result = pages[i].lower()\n",
    "        result = re.sub('[^0-9a-z]+', ' ', result) # replace non-alphanumeric chars with space\n",
    "        result = result.split(' ')\n",
    "\n",
    "        stopwords_set = set(stopwords.words('english'))\n",
    "        result = [w for w in result if w.isalpha()] # remove non-letter tokens\n",
    "        result = [w for w in result if len(w) >= 2] # remove tokens below a certain length\n",
    "        result = [w for w in result if w not in stopwords_set] # remove stopwords\n",
    "        # TODO stemming\n",
    "\n",
    "        processed_docs[i] = ' '.join(result)\n",
    "    return processed_docs\n",
    "\n",
    "docs = preprocess(pages) # docs have same indices as pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vectorizer(max_df = 0.9, min_df = 1, max_features = None):\n",
    "    \"\"\"\n",
    "    return:\n",
    "        tf-idf vectorizer\n",
    "    \"\"\"\n",
    "    return TfidfVectorizer(max_df = max_df, min_df = min_df, max_features = max_features)\n",
    "\n",
    "tfidf_vectorizer = get_tfidf_vectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(list(docs.values())).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_vector(query, tfidf_vectorizer):\n",
    "    \"\"\"\n",
    "    [query]: string\n",
    "    [tfidf_vectorizer]: tfidf vectorizer after fit_transform\n",
    "    return:\n",
    "        1d numpy.array of length = num_features(tfidf_vectorizer) representing the query as binary vector\n",
    "    \"\"\"\n",
    "    # TODO non-binary representation of query vector\n",
    "    features = tfidf_vectorizer.get_feature_names()\n",
    "    inv_idx = {t:i for (i,t) in enumerate(features)}\n",
    "    query_vec = np.zeros((len(features), ))\n",
    "    for w in query.split(' '):\n",
    "        try:\n",
    "            query_vec[inv_idx[w]] = 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if not np.any(query_vec): # query vector is all zeros\n",
    "        print('invalid query') # TODO better way to notify user\n",
    "    return query_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sim(query_vec, tfidf_matrix):\n",
    "    \"\"\"\n",
    "    [query_vec]: query vector of shape (num_features, )\n",
    "    [tfidf_matrix]: tf-idf matrix of shape (num_docs, num_features)\n",
    "    return:\n",
    "        1d numpy array of shape (num_docs, ) containing cosine similarity scores for query with each doc\n",
    "    note: norm(query) is removed from equation since it's constant for all docs\n",
    "    \"\"\"\n",
    "    norms_docs = np.linalg.norm(tfidf_matrix, axis = 1)\n",
    "    dot_prods = np.dot(tfidf_matrix, query_vec)\n",
    "    return dot_prods / norms_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_page_nums(cos_sims, top = 10):\n",
    "    \"\"\"\n",
    "    [docs]: document dictionary {page_num: page_text_string}\n",
    "    [cos_sims]: cosine similarity scores of shape (num_docs, )\n",
    "    [top]: how many top results are returned\n",
    "    return:\n",
    "        [rankings]: ranked list of page numbers based on similarity\n",
    "        [scores]: cosine similarity scores\n",
    "    \"\"\"\n",
    "    inds = np.argsort(cos_sims)[-top:][::-1]\n",
    "    rankings = np.array(list(docs.keys()))[inds]\n",
    "    scores = cos_sims[inds]\n",
    "    return (rankings, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(18, 0.5733842783319805)\n(19, 0.3181071361441875)\n(20, 0.3175908459382807)\n(437, 0.27549381331335565)\n(63, 0.2553825998026559)\n(203, 0.23825996680561531)\n(16, 0.2035434348548798)\n(10, 0.18878403232224875)\n(21, 0.1813544599727478)\n(210, 0.17317707032765625)\n"
     ]
    }
   ],
   "source": [
    "query = 'many years ago the nursing profession'\n",
    "q = get_query_vector(query, tfidf_vectorizer)\n",
    "cos_sims = get_cosine_sim(q, tfidf_matrix)\n",
    "(rankings, scores) = get_ranked_page_nums(cos_sims)\n",
    "for item in zip(rankings, scores):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use json to get paragraphs as documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}